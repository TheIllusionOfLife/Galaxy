# Galaxy Development Plan - October 29, 2025

**Status:** Active Development Plan
**Last Updated:** 2025-10-29 18:30 JST
**Planning Sources:**
- `/plan_next` command analysis (README, PRs, issues, ARCHITECTURE.md)
- 5 external reviews (CH, CL, GE, CO, GCL)
- Project vision (big_picture.md)

---

## Executive Summary

**Current State:**
- ✅ Core evolution engine functional (PR #16, #14 merged)
- ✅ Visualization system complete with token tracking
- ✅ Code length penalty system implemented (validation pending)
- ⚠️ **CRITICAL BUG FOUND**: elite_ratio configuration ignored (hardcoded 0.2)
- ⚠️ Security improvements needed (pre-commit hooks, secret scanning)

**Strategic Focus:**
1. **Immediate**: Fix implementation bugs, security hardening
2. **Short-term**: Validate penalty system, improve UX
3. **Mid-term**: Code modularization, enhanced visualizations
4. **Long-term**: Advanced algorithms, multi-LLM support, new domains

**Consensus from Reviews:**
- All 5 reviewers identified security/testing gaps (unanimous priority)
- 4/5 reviewers called for prototype.py modularization
- CO found critical elite_ratio bug (blocks current functionality)

---

## Immediate Actions (This Session - 2 hours)

### Priority 1: Bug Fixes (30 minutes)

#### 1.1 Fix elite_ratio Configuration Bug 🚨
**Source:** CO review
**Issue:** `prototype.py:502` hardcodes `elite_ratio=0.2`, ignoring `config.py:75` setting
**Impact:** User config changes silently fail

**Implementation:**
```python
# prototype.py:502
# Before: n_elites = max(1, int(len(self.population) * 0.2))
# After:  n_elites = max(1, int(len(self.population) * self.elite_ratio))
```

**Testing:**
- Unit test: Verify elite_ratio=0.3 selects 3/10 models
- Integration test: Config changes affect selection

**Files:**
- `prototype.py` (1 line change)
- `tests/test_evolution.py` (new test)

---

#### 1.2 Fix compile_external_surrogate Security Bypass
**Source:** CO review
**Issue:** Function skips AST validation when reusing generated code
**Impact:** Potential security vulnerability

**Implementation:**
- Always validate through CodeValidator, even for cached code
- Add validation to compile_external_surrogate flow

---

### Priority 2: Security Hardening (30 minutes)

#### 2.1 Pre-commit Hook for Secrets
**Source:** CH, CL consensus
**Implementation:**
```yaml
# .pre-commit-config.yaml (add)
- repo: https://github.com/Yelp/detect-secrets
  rev: v1.4.0
  hooks:
    - id: detect-secrets
      args: ['--baseline', '.secrets.baseline']
```

**Files to protect:**
- `*.env*`
- `*.backup`
- `*.bak`
- Any files with `API_KEY` patterns

---

#### 2.2 Enhanced .gitignore
**Source:** CH, CL
**Add:**
```gitignore
# Secrets
*.env*
!.env.example
*.backup
*.bak
*_BACKUP_*

# API keys
*apikey*
*api_key*
```

---

### Priority 3: UX Improvements (30 minutes)

#### 3.1 Matplotlib Missing Warning
**Source:** CO review
**Current:** Silent skip when matplotlib unavailable
**Fix:**
```python
# prototype.py or visualization.py
try:
    import matplotlib
except ImportError:
    print("⚠️  matplotlib not installed. Visualizations will be skipped.")
    print("   Install with: uv pip install matplotlib>=3.5.0")
```

---

#### 3.2 Cost Overrun CLI Notification
**Source:** CO review
**Current:** Only logs when switching to mock mode
**Fix:**
```python
# prototype.py:200
if cost_tracker.check_budget_exceeded():
    print("⚠️  BUDGET EXCEEDED - Switching to mock mode")
    print(f"   Spent: ${cost_tracker.total_cost_usd:.4f}")
    # Add flag to evolution_history.json
    history_entry["fallback_mode"] = "mock"
```

---

### Priority 4: Code Length Penalty Validation (30 minutes)
**Source:** Original /plan_next Priority #1
**Status:** Implementation ✅, Baseline ✅, Testing ⏳

**Execution Plan:**
```bash
# Run 3 comparative tests
# Weight 0.05 (conservative)
cp .env .env.backup
echo "ENABLE_CODE_LENGTH_PENALTY=true" >> .env
echo "CODE_LENGTH_PENALTY_WEIGHT=0.05" >> .env
uv run python prototype.py

# Weight 0.1 (default)
sed -i '' 's/PENALTY_WEIGHT=0.05/PENALTY_WEIGHT=0.1/' .env
uv run python prototype.py

# Weight 0.2 (aggressive)
sed -i '' 's/PENALTY_WEIGHT=0.1/PENALTY_WEIGHT=0.2/' .env
uv run python prototype.py

# Analysis
ls -la results/run_*/token_progression.png
```

**Success Criteria:**
- Higher penalty weights → lower token counts
- Fitness remains acceptable (>80% of baseline)
- Clear trend in token_progression.png

---

## Short-Term Tasks (Next 1-2 Sessions - 6 hours)

### Session 2: Testing & Documentation (3 hours)

#### 2.1 Integration Test Improvements
**Source:** CL, CO, GE consensus

**Current Issues:**
- Empty integration tests with `pass`
- Tests skip by default (`-m "not integration"`)

**Implementation:**
1. **Mock-based smoke tests** (CO suggestion):
```python
# tests/test_integration_smoke.py
def test_evolution_loop_mock(mock_gemini_client):
    """Fast smoke test with mocked LLM responses"""
    engine = EvolutionaryEngine(...)
    engine.run_evolutionary_cycle()
    assert len(engine.population) == POPULATION_SIZE
```

2. **Remove or implement empty tests** (CL suggestion):
- Either implement with assertions
- Or remove placeholder `pass` tests

**Files:**
- `tests/test_integration.py` (enhance)
- `tests/test_integration_smoke.py` (new)

---

#### 2.2 Documentation Enhancements
**Source:** CH, CL

**CONTRIBUTING.md Updates:**
```markdown
## Development Workflow
1. Create feature branch: `git checkout -b feature/description`
2. Run tests: `make check`
3. Add tests for new features (TDD)
4. Commit at logical milestones
5. Create PR with descriptive title
```

**README.md Impact Statement** (CH suggestion):
```markdown
## Why Galaxy? Why N-body Simulation?

Galaxy-scale gravitational N-body simulation remains one of physics'
computationally hardest problems. Traditional methods require hours of
supercomputer time. Galaxy lets LLM civilizations **invent their own
approximation theories** — testing whether AI can independently discover
scientific methods that surpass human-designed algorithms.

This is not just optimization. It's cognitive emergence: AI creating
knowledge humans haven't discovered yet.
```

---

### Session 3: Visualization Enhancements (3 hours)

#### 3.1 Best-So-Far Tracking
**Source:** CH review (highest priority visualization)

**Implementation:**
```python
# visualization.py
def plot_fitness_progression_with_best(history, output_path):
    """Enhanced plot with best-ever fitness tracking"""
    generations = []
    best_fitness = []
    avg_fitness = []
    worst_fitness = []
    best_ever = 0.0

    for entry in history:
        # ... existing logic ...
        best_ever = max(best_ever, entry["best_fitness"])
        best_fitness.append(best_ever)  # Monotonic line

    # Plot best-ever as separate line
    ax.plot(generations, best_fitness, 'g-', linewidth=2,
            label='Best Ever', zorder=10)
```

**Benefits:**
- Shows cumulative progress (vs non-monotonic generation best)
- Easier to explain to stakeholders
- Validates elitism works

---

#### 3.2 Pareto Front Visualization
**Source:** CH review

**Implementation:**
```python
# visualization.py
def plot_pareto_front(history, output_path):
    """Plot Pareto frontier of accuracy vs speed"""
    all_models = []
    for entry in history:
        all_models.extend(entry["population"])

    # Extract non-dominated solutions
    pareto_front = compute_pareto_front(all_models,
                                        objectives=["accuracy", "speed"])

    # Plot all models + highlight Pareto front
    plt.scatter(speeds, accuracies, alpha=0.3, label="All Models")
    plt.scatter(pareto_speeds, pareto_accuracies,
                color='red', s=100, label="Pareto Front")
```

**Use Case:**
- Identify models with best accuracy/speed trade-offs
- Validate multi-objective optimization (future work)

---

## Mid-Term Tasks (1-2 Weeks - 12 hours)

### 3.1 Code Modularization
**Source:** CH, CL, GE, CO consensus (4/5 reviewers)

**Current Problem:**
- `prototype.py` mixes evolution loop, evaluation, and coordination
- Hard to extend to new domains (fluid dynamics, etc.)

**Proposed Structure:**
```
galaxy/
├── core/
│   ├── genome.py          # SurrogateGenome dataclass
│   ├── evolution.py       # EvolutionaryEngine
│   └── selection.py       # Elite selection strategies
├── crucible/
│   ├── base.py            # Abstract Crucible class
│   ├── nbody.py           # CosmologyCrucible (current)
│   └── fluid.py           # FluidDynamicsCrucible (future)
├── llm/
│   ├── client.py          # Abstract LLMClient
│   ├── gemini.py          # GeminiClient
│   └── anthropic.py       # ClaudeClient (future)
└── prototype.py           # Main orchestration (100 lines)
```

**Migration Strategy:**
1. Extract SurrogateGenome → `core/genome.py`
2. Extract EvolutionaryEngine → `core/evolution.py`
3. Extract CosmologyCrucible → `crucible/nbody.py`
4. Update imports, run full test suite
5. Commit after each extraction (semantic commits)

**Benefits:**
- SOLID principles (single responsibility)
- Easy to add new domains (ARCHITECTURE.md:1171-1199)
- Testable in isolation

---

### 3.2 Prompt Engineering - Sub-1% Error Rate
**Source:** Original /plan_next Priority #2, GE review

**Current:** 1.67% syntax error rate (target: <1%)

**Approach:**
1. **Analyze failure patterns** from production runs:
   - What triggers errors? (long code, complex logic, specific patterns)
   - Which generation phase? (explore vs exploit)
   - Common error types? (unclosed brackets, incomplete functions)

2. **Targeted prompt improvements:**
```python
# prompts.py - enhanced for explore phase (temp 1.0)
EXPLORE_SAFETY_REMINDER = """
⚠️ CRITICAL - Exploration Phase Checklist:
1. If your code exceeds 1500 characters, SIMPLIFY before submitting
2. Count opening/closing: ( ) [ ] { } must match EXACTLY
3. Test mentally: Can this run without imports/globals?
4. When in doubt, choose SIMPLER over CLEVER
"""
```

3. **Validation:**
   - 3 full runs (180 API calls, $0.06)
   - Target: 0/180 errors (0%) or max 1/180 (0.56%)

---

### 3.3 Tiktoken Migration
**Source:** CL, GCL, GE consensus

**Current:** Whitespace-based token counting (approximation)
**Target:** tiktoken for accurate OpenAI-compatible counting

**Implementation:**
```python
# prototype.py or new utils/tokenizer.py
try:
    import tiktoken

    def count_tokens(code: str) -> int:
        enc = tiktoken.encoding_for_model("gpt-4")
        return len(enc.encode(code))
except ImportError:
    # Fallback to whitespace
    def count_tokens(code: str) -> int:
        return len(code.split())
```

**Benefits:**
- Accurate penalty calculation
- Comparable across LLM providers
- Industry-standard metric

---

## Long-Term Tasks (1-2 Months - 20+ hours)

### 4.1 Multi-LLM Provider Support
**Source:** Original /plan_next Priority #4, CL review

**Implementation:**
```python
# llm/base.py
class LLMClient(ABC):
    @abstractmethod
    def generate_code(self, prompt: str) -> LLMResponse:
        pass

# llm/anthropic.py
class ClaudeClient(LLMClient):
    def generate_code(self, prompt: str) -> LLMResponse:
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            messages=[{"role": "user", "content": prompt}]
        )
        # Extract code, calculate cost, return LLMResponse
```

**Comparative Experiment:**
- Same prompts, different providers (Gemini vs Claude vs GPT)
- Compare: syntax error rate, fitness quality, cost
- Document findings

---

### 4.2 Advanced Evolution Algorithms
**Source:** GCL review (research proposals)

#### 4.2.1 Genetic Crossover
**Status:** Research hypothesis (needs validation)

**Implementation:**
```python
# prompts.py (already has get_crossover_prompt stub)
def get_crossover_prompt(parent1_code, parent2_code, fitness1, fitness2):
    return f"""
    PARENT 1 (Fitness: {fitness1}):
    {parent1_code}

    PARENT 2 (Fitness: {fitness2}):
    {parent2_code}

    Generate a HYBRID that combines the strengths of both approaches.
    """

# core/evolution.py
def breed_next_generation_with_crossover(elites):
    for i in range(population_size):
        if random.random() < crossover_rate:  # e.g., 0.3
            parent1, parent2 = random.sample(elites, 2)
            child = LLM_crossover(parent1, parent2)
        else:
            child = LLM_mutate(random.choice(elites))
```

**Validation Required:**
- A/B test: crossover vs mutation-only
- Measure: fitness improvement rate, diversity metrics
- Cost: 2-3 full runs ($0.06-0.09)

---

#### 4.2.2 Dynamic Temperature Adjustment
**Status:** Enhancement to existing explore/exploit strategy

**Implementation:**
```python
# config.py
class Settings(BaseSettings):
    # ... existing ...
    convergence_threshold: float = 0.05  # 5% improvement
    convergence_window: int = 3  # generations
    stagnation_temp_boost: float = 0.3  # add to current temp

# core/evolution.py
def get_adaptive_temperature(generation, fitness_history):
    base_temp = self.get_mutation_temperature(generation)

    # Check for stagnation
    if len(fitness_history) >= convergence_window:
        recent = fitness_history[-convergence_window:]
        improvement = (max(recent) - min(recent)) / min(recent)

        if improvement < convergence_threshold:
            # Stagnated - boost exploration
            return min(2.0, base_temp + stagnation_temp_boost)

    return base_temp
```

---

#### 4.2.3 Multi-Objective Optimization
**Source:** GCL review

**Implementation:**
```python
# crucible/nbody.py
def evaluate_surrogate_model(surrogate_func):
    # ... existing accuracy, speed ...

    # New: Energy conservation metric
    energy_error = compute_energy_conservation_error(scenarios)

    return {
        "accuracy": accuracy,
        "speed": speed,
        "energy_conservation": 1.0 - energy_error  # Higher is better
    }

# core/selection.py
def select_pareto_optimal(population, objectives=["accuracy", "speed"]):
    """Select non-dominated solutions"""
    pareto_front = []
    for candidate in population:
        dominated = False
        for other in population:
            if dominates(other, candidate, objectives):
                dominated = True
                break
        if not dominated:
            pareto_front.append(candidate)
    return pareto_front
```

**Use Case:**
- Discover models with high accuracy, speed, AND physical correctness
- Avoid over-fitting to speed/accuracy at cost of physics violations

---

### 4.3 Self-Correction from Validation Errors
**Source:** GCL review

**Implementation:**
```python
# prototype.py - LLM_propose_surrogate_model
def LLM_propose_surrogate_model(...):
    max_retries = 2

    for attempt in range(max_retries):
        response = gemini_client.generate_code(prompt)
        compiled_func, validation = validate_and_compile(response.code)

        if validation.valid:
            return SurrogateGenome(...)

        # Self-correction attempt
        if attempt < max_retries - 1:
            error_feedback = "\n".join(validation.errors)
            prompt = get_error_correction_prompt(
                original_code=response.code,
                errors=error_feedback
            )
            # Retry with corrected prompt

    # Final fallback
    return _mock_surrogate_generation(...)
```

**Benefits:**
- Learn from mistakes (meta-learning)
- Reduce fallback to mock mode
- Improve LLM code generation quality

---

### 4.4 Convergence Detection
**Source:** Original /plan_next Priority #3

**Implementation:**
```python
# core/evolution.py
def should_stop_evolution(self, generation, fitness_history):
    # Check 1: Budget exhausted
    if self.cost_tracker.check_budget_exceeded():
        return True, "budget_exceeded"

    # Check 2: Max generations
    if generation >= self.num_generations:
        return True, "max_generations"

    # Check 3: Fitness plateau
    if len(fitness_history) >= 3:
        recent_best = fitness_history[-3:]
        improvement = (max(recent_best) - min(recent_best)) / min(recent_best)
        if improvement < 0.05:  # <5% improvement
            return True, "converged"

    return False, None
```

---

## Deferred / Research Items

### Not Implementing (Rationale)

#### D.1 Distributed Execution
**Source:** CL review
**Reason:** YAGNI - current scale (50 API calls, 4 min runtime) doesn't justify complexity
**Reconsider when:** Running 1000+ population, multi-hour runs

#### D.2 CODEOWNERS File
**Source:** CL review
**Reason:** Overkill for solo/duo research project
**Reconsider when:** 5+ regular contributors

#### D.3 New Problem Domains (Immediate)
**Source:** ARCHITECTURE.md Extension Point #3
**Reason:** Need to validate N-body prototype first
**Next domain candidates:**
- Fluid dynamics (CFD acceleration)
- Molecular dynamics
- Combinatorial optimization

**Prerequisites:**
- N-body surrogate achieves >0.95 accuracy reliably
- Code modularization complete (pluggable Crucible)
- Multi-objective optimization working

---

## Risk Management

### Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| LLM API rate limit exceeded | Low | High | Rate limiter enforces 15 RPM |
| Code length penalty over-penalizes | Medium | Medium | A/B test multiple weights before recommending |
| Crossover reduces diversity | Medium | Medium | Empirical validation before adopting |
| tiktoken import fails | Low | Low | Graceful fallback to whitespace counting |

### Process Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Secret leakage to git | Low | Critical | Pre-commit hooks, secret scanning CI |
| Test coverage regression | Medium | Medium | Coverage threshold in CI (maintain >60%) |
| Breaking changes during refactor | Medium | High | Modularize incrementally, commit after each extraction |

---

## Success Metrics

### Short-Term (1-2 weeks)
- ✅ Zero config bugs (elite_ratio fix validated)
- ✅ Zero security incidents (pre-commit hooks working)
- ✅ Penalty system validated (optimal weight documented)
- ✅ Error rate <1% (3 runs with 0-1 errors)

### Mid-Term (1-2 months)
- ✅ Code modularization complete (5+ modules)
- ✅ Test coverage >70% (up from 67%)
- ✅ Visualization suite expanded (best-ever, Pareto front)
- ✅ tiktoken integration complete

### Long-Term (3-6 months)
- ✅ Multi-LLM support (Gemini + Claude working)
- ✅ Advanced algorithms validated (crossover, multi-objective)
- ✅ Second problem domain operational (fluid or molecular dynamics)
- 🎯 **Ultimate Goal:** Surrogate model outperforms human-designed algorithms

---

## Alignment with Project Vision

From `big_picture.md`:

> **プロジェクトの唯一かつ究極の目的は、「認知的創発（Cognitive Emergence）」を達成することである。**

**How this plan supports cognitive emergence:**

1. **Short-term fixes** → Stable foundation for experimentation
2. **Visualization enhancements** → Observe emergence patterns scientifically
3. **Advanced algorithms** → Create conditions for unexpected discoveries
4. **Multi-domain expansion** → Test if strategies transfer (true intelligence)
5. **Self-correction** → Meta-learning capability (step toward autonomy)

**First Success Criterion:**
> 特定の問題領域において、既知の人類が設計したアルゴリズムよりも優れた性能を持つ、全く新しいアルゴリズムを設計・提示する。

**Pathway:**
- Code length penalty → Encourages elegant, not complex solutions
- Multi-objective → Discover physically grounded approximations
- Crossover → Combine disparate insights into novel approaches
- Self-correction → Learn from failures autonomously

---

## Execution Order (Recommended)

### Week 1
1. **Day 1 (Today):** Immediate actions (bugs, security, UX, penalty testing)
2. **Day 2-3:** Integration tests, documentation enhancements
3. **Day 4-5:** Visualization enhancements (best-ever, Pareto front)

### Week 2
4. **Day 1-3:** Code modularization (genome, evolution, crucible)
5. **Day 4-5:** Prompt engineering iteration, tiktoken migration

### Month 2
6. **Week 1-2:** Multi-LLM support (Claude integration)
7. **Week 3:** Advanced algorithms (crossover experimentation)
8. **Week 4:** Convergence detection, dynamic temperature

### Month 3+
9. **Research:** Multi-objective optimization validation
10. **Research:** Self-correction implementation
11. **Expansion:** Second problem domain (TBD based on results)

---

## Notes & Open Questions

### Questions for Future Sessions
1. **Crossover value:** Does LLM-based crossover actually improve fitness vs mutation-only?
2. **Optimal elite ratio:** Is 0.2 actually optimal, or should we experiment with 0.1, 0.3?
3. **Temperature schedule:** Can we learn optimal temperature curves from past runs?
4. **Domain transfer:** Will N-body strategies help with fluid dynamics, or start from scratch?

### Dependencies to Watch
- **Gemini API stability:** Free tier has been reliable so far
- **tiktoken compatibility:** Check if it handles non-English variable names
- **matplotlib version:** Ensure visualization works across Python 3.10-3.12

### Metrics to Track
- LLM syntax error rate (target: <1%)
- Average tokens per generation (target: <2000 with penalty)
- Best fitness per run (target: >20,000 by Gen 5)
- Cost per run (target: <$0.03)

---

## Document History

| Date | Author | Changes |
|------|--------|---------|
| 2025-10-29 | Claude Code | Initial plan synthesizing /plan_next + 5 reviews |

---

**Next Review:** After completing Week 1 tasks, reassess priorities based on:
- Penalty testing results (optimal weight)
- Integration test findings (new failure modes?)
- Community feedback (if open-sourced)
