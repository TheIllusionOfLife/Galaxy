# Comprehensive Plan: Gemini 2.5 Flash Lite Integration

**Date**: 2025-10-24
**Branch**: feature/gemini-integration
**Model**: gemini-2.5-flash-lite

## Why Gemini 2.5 Flash Lite is Perfect for This Project

**Key Advantages:**
- ✅ **FREE tier** - 1,000 requests/day, 15 requests/minute
- ✅ **30-40x cheaper** than Claude if paid tier needed ($0.10 vs $3.00 per 1M input tokens)
- ✅ **Fast & lightweight** - "cost efficiency and low latency" optimized
- ✅ **Structured outputs** - Can enforce JSON/code format strictly
- ✅ **Code execution** - Built-in capability for testing code
- ✅ **1M token context** - Can include extensive examples in prompts

**For your use case (50 LLM calls per run):**
- Free tier: 20 full experiments per day
- Runtime: ~3.5 minutes per run (15 RPM limit)
- Cost if paid: $0.01-0.02 per run (vs $2-3 with Claude)
- **Perfectly aligned with resource constraint requirement**

---

## Phase 1: Project Infrastructure

### 1.1 Create `pyproject.toml`
```toml
[project]
name = "galaxy-prometheus"
version = "0.1.0"
description = "Evolutionary surrogate model discovery with Gemini 2.5 Flash Lite"
requires-python = ">=3.10"
dependencies = [
    "google-generativeai>=0.8.0",
    "python-dotenv>=1.0.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "mypy>=1.0.0",
]

claude = [
    "anthropic>=0.18.0",  # Optional for comparison experiments
]
```

### 1.2 Create `.env.example`
```bash
# Google AI API Key (get free key at https://aistudio.google.com/apikey)
GOOGLE_API_KEY=your_api_key_here

# Optional: Anthropic for comparison experiments
# ANTHROPIC_API_KEY=sk-ant-xxxxx

# Model Configuration
LLM_MODEL=gemini-2.5-flash-lite
TEMPERATURE=0.8
MAX_OUTPUT_TOKENS=2000

# Cost Control (free tier: 1000 requests/day)
MAX_REQUESTS_PER_RUN=50
ENABLE_RATE_LIMITING=true

# Evolution Parameters
POPULATION_SIZE=10
NUM_GENERATIONS=5
ELITE_RATIO=0.2

# Mutation Strategy
EARLY_MUTATION_TEMP=1.0  # Generations 0-2: high creativity
LATE_MUTATION_TEMP=0.6   # Generations 3+: refinement
```

### 1.3 Create `.gitignore`
```gitignore
# Secrets
.env
*.key

# Python
.venv/
venv/
__pycache__/
*.pyc
*.pyo

# Generated files
evolution_history.json
logs/
*.log
generated_code/

# IDE
.vscode/
.idea/
*.swp
.DS_Store

# Experiments
experiments/
results/
```

### 1.4 Create `config.py`
```python
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field
from typing import Literal

class Settings(BaseSettings):
    """Configuration from environment variables"""

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore"
    )

    # API Configuration
    google_api_key: str = Field(..., description="Google AI API key")
    anthropic_api_key: str | None = Field(None, description="Optional Claude API key")

    # Model Selection
    llm_model: str = Field("gemini-2.5-flash-lite", description="Gemini model to use")
    temperature: float = Field(0.8, ge=0.0, le=2.0)
    max_output_tokens: int = Field(2000, ge=100, le=8192)

    # Rate Limiting (Free tier: 15 RPM, 1000 RPD)
    max_requests_per_run: int = Field(50, description="Max LLM calls per run")
    enable_rate_limiting: bool = Field(True)
    requests_per_minute: int = Field(15, description="Free tier: 15 RPM")

    # Evolution
    population_size: int = Field(10, ge=1, le=100)
    num_generations: int = Field(5, ge=1, le=100)
    elite_ratio: float = Field(0.2, ge=0.0, le=1.0)

    # Mutation
    early_mutation_temp: float = Field(1.0, ge=0.0, le=2.0)
    late_mutation_temp: float = Field(0.6, ge=0.0, le=2.0)

    @property
    def total_requests_needed(self) -> int:
        """Total LLM calls needed for one run"""
        return self.population_size * self.num_generations

    @property
    def estimated_runtime_minutes(self) -> float:
        """Estimated runtime based on rate limit"""
        if not self.enable_rate_limiting:
            return 0.5  # Assume ~30 sec without rate limiting
        return self.total_requests_needed / self.requests_per_minute

    def get_mutation_temperature(self, generation: int) -> float:
        """Temperature schedule: high early, low later"""
        if generation < 3:
            return self.early_mutation_temp
        return self.late_mutation_temp

# Global config instance
settings = Settings()
```

---

## Phase 2: Gemini Client Implementation

### 2.1 Create `gemini_client.py`

**Core Components:**

#### 2.1.1 LLMResponse Dataclass
```python
@dataclass
class LLMResponse:
    code: str
    raw_response: str
    tokens_used: int
    cost_usd: float
    model: str
    success: bool
    error: Optional[str] = None
    generation_time_s: float = 0.0
```

#### 2.1.2 RateLimiter
- Enforces 15 requests per minute (free tier)
- Tracks last request time
- Sleeps if needed to maintain rate

#### 2.1.3 GeminiClient
**Key Features:**
- Uses `google.generativeai` library
- Pricing: $0.10/1M input tokens, $0.40/1M output tokens
- Safety settings: Block none (allow code generation)
- Exponential backoff retry (3 attempts)
- Token counting via `usage_metadata`
- Extracts code from markdown blocks or raw text

**Methods:**
- `generate_surrogate_code(prompt, retry_attempts=3) -> LLMResponse`
- `_extract_code(text) -> str`
- `_calculate_cost(prompt_tokens, completion_tokens) -> float`

#### 2.1.4 CostTracker
**Features:**
- Tracks total cost across run
- Records each API call
- Budget enforcement
- Summary statistics

**Methods:**
- `add_call(response, context)`
- `check_budget_exceeded() -> bool`
- `get_summary() -> Dict`

---

## Phase 3: Prompt Engineering for Gemini

### 3.1 Create `prompts.py`

#### 3.1.1 System Instruction
```python
SYSTEM_INSTRUCTION = """You are an expert in numerical methods and physics simulation.
Generate Python code for a surrogate model that approximates N-body gravitational dynamics.

CRITICAL REQUIREMENTS:
1. Define EXACTLY this function signature:
   def predict(particle, attractor):
       # Your code here
       return [new_x, new_y, new_vx, new_vy]

2. Input format:
   - particle: [x, y, vx, vy] (position x, y and velocity vx, vy)
   - attractor: [ax, ay] (central gravity source position)

3. Output: [new_x, new_y, new_vx, new_vy] (next state after one timestep)

4. Physics: Approximate gravity (force ∝ 1/r² or similar)

5. Constraints:
   - Use ONLY: math module (math.sqrt, math.sin, etc.), basic Python (abs, min, max)
   - NO imports, NO loops over large arrays, NO recursion
   - Fast and simple (will be called 50 times per evaluation)

6. Output ONLY the Python code, no markdown, no explanations."""
```

#### 3.1.2 Prompt Functions
- `get_initial_prompt(seed: int) -> str`
  - 6 different approaches (Euler, semi-implicit, polynomial, damping, Verlet, softened gravity)
  - Rotates based on seed for diversity

- `get_mutation_prompt(parent_code, fitness, accuracy, speed, generation, mutation_type) -> str`
  - Early generations: "explore" - try different approaches
  - Late generations: "exploit" - refine existing approach
  - Includes parent performance metrics
  - Adaptive strategy suggestions

- `get_structured_output_schema() -> dict` (optional)
  - Leverages Gemini's structured output feature
  - Enforces response format

---

## Phase 4: Code Validation & Safety

### 4.1 Create `code_validator.py`

**Multi-layer Validation:**

#### 4.1.1 Static Analysis (AST)
- Parse syntax
- Check for `predict` function
- Validate signature (2 arguments)
- Block forbidden operations:
  - Imports (ast.Import, ast.ImportFrom)
  - Async (ast.AsyncFunctionDef)
  - Global variables
  - Dangerous builtins (eval, exec, compile, open)
  - Infinite loops (while True without break)

#### 4.1.2 Complexity Warnings
- Count loops (warn if >2)
- Count recursion depth
- Estimate computational cost

#### 4.1.3 Safe Compilation
```python
def compile_safely(code: str, attractor: list[float]) -> Callable | None:
    # Create sandbox with limited builtins
    # exec() in restricted namespace
    # Extract predict function
    # Test with sample input
    # Validate output format (4 numeric values)
    # Check for NaN/Inf
    # Return wrapped function or None
```

**ValidationResult dataclass:**
- `valid: bool`
- `errors: list[str]`
- `warnings: list[str]`

---

## Phase 5: Integration into prototype.py

### 5.1 Critical Changes

#### 5.1.1 Fix Type Annotation Bug (Line 158)
```python
# Before:
def evaluate_surrogate_model(...) -> (float, float):

# After:
def evaluate_surrogate_model(...) -> tuple[float, float]:
```

#### 5.1.2 Add Imports
```python
import logging
from config import settings
from gemini_client import GeminiClient, CostTracker, LLMResponse
from prompts import get_initial_prompt, get_mutation_prompt
from code_validator import CodeValidator

logger = logging.getLogger(__name__)
```

#### 5.1.3 Update SurrogateGenome
```python
@dataclass
class SurrogateGenome:
    theta: List[float]
    description: str = "parametric"
    raw_code: Optional[str] = None
    fitness: Optional[float] = None  # NEW: Store for prompt context
```

#### 5.1.4 Refactor LLM_propose_surrogate_model()
```python
def LLM_propose_surrogate_model(
    base_genome: Optional[SurrogateGenome],
    generation: int,
    gemini_client: Optional[GeminiClient] = None,
    cost_tracker: Optional[CostTracker] = None
) -> SurrogateGenome:
    """
    Generate using Gemini or fallback to mock

    Logic:
    1. If no client -> mock mode
    2. If budget exceeded -> mock mode
    3. Generate prompt (initial vs mutation)
    4. Call Gemini API
    5. Validate code
    6. Return genome with LLM code or fallback to mock
    """
```

**Helper function:**
```python
def _mock_surrogate_generation(
    base_genome: Optional[SurrogateGenome],
    generation: int
) -> SurrogateGenome:
    """Current parametric mutation logic as fallback"""
```

#### 5.1.5 Update EvolutionaryEngine
```python
class EvolutionaryEngine:
    def __init__(
        self,
        crucible: CosmologyCrucible,
        population_size: int = 10,
        gemini_client: Optional[GeminiClient] = None,
        cost_tracker: Optional[CostTracker] = None
    ):
        # NEW: Store LLM client and cost tracker
        self.gemini_client = gemini_client
        self.cost_tracker = cost_tracker or CostTracker()
        self.history: List[Dict] = []  # NEW: Evolution history
```

**Method updates:**
- `initialize_population()`: Pass gemini_client and cost_tracker to LLM function
- `run_evolutionary_cycle()`: Store fitness in genome after evaluation
- Breeding: Pass parent genome with fitness to LLM function

#### 5.1.6 Update Main Block
```python
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Initialize Gemini client (or None for mock)
    gemini_client = None
    if settings.google_api_key:
        logger.info(f"Initializing Gemini: {settings.llm_model}")
        gemini_client = GeminiClient(
            api_key=settings.google_api_key,
            model=settings.llm_model,
            temperature=settings.temperature,
            max_output_tokens=settings.max_output_tokens,
            enable_rate_limiting=settings.enable_rate_limiting
        )
        logger.info(f"Estimated runtime: {settings.estimated_runtime_minutes:.1f} min")
    else:
        logger.info("No API key, using mock mode")

    # Initialize cost tracker
    cost_tracker = CostTracker(max_cost_usd=1.0)

    # Create and run evolution
    crucible = CosmologyCrucible()
    engine = EvolutionaryEngine(
        crucible,
        population_size=settings.population_size,
        gemini_client=gemini_client,
        cost_tracker=cost_tracker
    )

    engine.initialize_population()

    for gen in range(settings.num_generations):
        engine.run_evolutionary_cycle()
        if gemini_client:
            summary = cost_tracker.get_summary()
            logger.info(f"Cost so far: ${summary['total_cost_usd']:.4f}")

    # Final summary
    if gemini_client:
        summary = cost_tracker.get_summary()
        logger.info("\n" + "="*50)
        logger.info("FINAL SUMMARY")
        logger.info(f"Total calls: {summary['total_calls']}")
        logger.info(f"Successful: {summary['successful_calls']}")
        logger.info(f"Total cost: ${summary['total_cost_usd']:.4f}")
        logger.info(f"Total tokens: {summary['total_tokens']:,}")
        logger.info(f"API time: {summary['total_time_s']:.1f}s")
        logger.info("="*50)
```

---

## Phase 6: Testing & Validation

### 6.1 Create `test_gemini_connection.py`
**Purpose:** Quick verification script

**Flow:**
1. Initialize Gemini client (no rate limiting for test)
2. Generate one surrogate model with initial prompt
3. Print response (tokens, cost, time, code)
4. Validate code (AST checks)
5. Compile and execute test call
6. Report success/failure

**Expected output:**
```
Testing Gemini 2.5 Flash Lite connection...
Calling Gemini API...
✅ API call successful!
Tokens used: 1234
Cost: $0.000543
Time: 1.23s

Generated code:
--------------------------------------------------
def predict(particle, attractor):
    ...
--------------------------------------------------

✅ Code passed validation
✅ Execution successful!
Input:  [45.0, 45.0, 1.0, 1.0]
Output: [46.05, 46.05, 1.05, 1.05]
```

---

## Phase 7: Documentation

### 7.1 Update `README.md`

**New sections:**
1. **Overview** - What this project does
2. **Why Gemini 2.5 Flash Lite** - Cost comparison, features
3. **Installation** - pip/uv commands
4. **Setup API Key** - Link to Google AI Studio
5. **Usage** - Basic run, mock mode, custom config
6. **Cost Estimates** - Free tier vs paid tier, comparison to Claude
7. **How It Works** - 5-step evolution process
8. **Output** - evolution_history.json format
9. **Troubleshooting** - Common issues and fixes

### 7.2 Create `GEMINI_GUIDE.md`

**Contents:**
- Gemini 2.5 Flash Lite specifications
- Prompt engineering best practices
- Rate limit handling strategies
- Cost optimization tips
- Structured output usage
- Comparison with other models (Claude, GPT)

---

## Implementation Timeline

### Week 1: Foundation (Days 1-3)
**Day 1** (3-4 hours):
- Create pyproject.toml
- Create .env.example
- Create .gitignore
- Create config.py
- Test config loading

**Day 2** (4-5 hours):
- Create gemini_client.py (RateLimiter, GeminiClient, CostTracker)
- Implement LLMResponse dataclass
- Add error handling and retry logic

**Day 3** (2-3 hours):
- Create prompts.py (system instruction, initial, mutation)
- Create test_gemini_connection.py
- Verify API works end-to-end

### Week 2: Integration (Days 4-6)
**Day 4** (4-5 hours):
- Create code_validator.py
- Implement AST validation
- Implement safe compilation
- Write validation tests

**Day 5** (4-5 hours):
- Fix prototype.py type annotation bug
- Add imports
- Update SurrogateGenome
- Refactor LLM_propose_surrogate_model()
- Add _mock_surrogate_generation()

**Day 6** (3-4 hours):
- Update EvolutionaryEngine
- Update main block
- Run first full evolution with Gemini
- Debug issues

### Week 3: Polish (Days 7-9)
**Day 7** (3-4 hours):
- Add detailed logging throughout
- Add history tracking (save to JSON)
- Improve error messages
- Test edge cases

**Day 8** (2-3 hours):
- Write unit tests (pytest)
- Test mock mode
- Test rate limiting
- Test budget enforcement

**Day 9** (2-3 hours):
- Update README.md
- Create GEMINI_GUIDE.md
- Add code comments
- Clean up code

**Total: ~9 days @ 3-4 hours/day = 27-36 hours**

---

## File Structure After Implementation

```
Galaxy/
├── .env                          # Secret (gitignored)
├── .env.example                  # Template
├── .gitignore                    # NEW
├── pyproject.toml                # NEW
├── config.py                     # NEW - Configuration
├── gemini_client.py              # NEW - Gemini API client
├── prompts.py                    # NEW - Prompt templates
├── code_validator.py             # NEW - Code validation
├── prototype.py                  # MODIFIED - Main evolution
├── test_gemini_connection.py     # NEW - Quick test script
├── README.md                     # MODIFIED - Updated docs
├── GEMINI_GUIDE.md               # NEW - Gemini-specific guide
├── plan_20251024.md              # THIS FILE
├── big_picture.md                # Existing
├── claude_improvement_plan.md    # Existing
├── codex_improvement_plan.md     # Existing
└── LICENSE                       # Existing
```

---

## Open Questions for Discussion

1. **Structured Output**: Should we use Gemini's structured output feature to enforce code format?
   - **Recommendation**: Start without, add if parsing issues arise

2. **Initial Diversity**: Use 6 different prompt templates (rotate by seed) or rely on temperature randomness?
   - **Recommendation**: Use 6 templates for guaranteed diversity

3. **Mutation Strategy**: Always use LLM, or hybrid (mock early, LLM later)?
   - **Recommendation**: All-LLM to test capabilities fully

4. **Error Recovery**: If LLM generates bad code 3x, fallback permanently or keep trying?
   - **Recommendation**: Keep trying with different prompts (vary temperature)

5. **Budget**: Default $1 max per run (safety net) or unlimited (rely on free tier)?
   - **Recommendation**: $1 limit as safety, but free tier sufficient for 50 calls

---

## Success Criteria

### Must Have (Definition of Done)
- ✅ Gemini 2.5 Flash Lite integration works end-to-end
- ✅ Rate limiting respects 15 RPM free tier
- ✅ Cost tracking accurate (verified against actual usage)
- ✅ Code validation catches unsafe patterns
- ✅ Evolution completes 5 generations without crashes
- ✅ History saved to evolution_history.json
- ✅ Documentation complete (README, setup)

### Nice to Have
- ✅ LLM models achieve higher fitness than mock models
- ✅ Generated code is readable and makes physical sense
- ✅ Cost per run < $0.05 (free tier)
- ✅ test_gemini_connection.py works reliably
- ✅ All validation tests pass

### Stretch Goals
- ✅ LLM discovers novel integration scheme
- ✅ Structured output feature implemented
- ✅ Visualization of evolution progress
- ✅ Benchmark vs classical methods (Verlet, RK4)
- ✅ Published results or paper

---

## Risk Mitigation

### Risk 1: LLM generates invalid code frequently
**Mitigation:**
- Multi-layer validation (AST + execution)
- Fallback to mock mode on repeated failures
- Log failures, analyze patterns, improve prompts
- Test with different temperatures

### Risk 2: Rate limiting too restrictive
**Mitigation:**
- 15 RPM = 3-4 minutes for 50 calls (acceptable)
- Can disable for paid tier
- Queue requests if needed
- Consider batch API (not implemented in v1)

### Risk 3: Free tier quota exceeded (1000 requests/day)
**Mitigation:**
- Monitor daily usage
- Alert when approaching limit
- 50 calls/run = 20 runs/day (generous for development)
- Fallback to mock if quota exceeded

### Risk 4: Generated code is slower than parametric
**Mitigation:**
- Fitness function naturally selects for speed
- LLM can see parent speed and optimize
- Keep mock mode for baseline comparison
- Document when LLM provides value vs overhead

### Risk 5: API connectivity issues
**Mitigation:**
- Exponential backoff retry (3 attempts)
- Clear error messages
- Graceful degradation to mock mode
- Save progress after each generation

---

## Cost Analysis

### Free Tier (First 1,000 requests/day)
- **Cost**: $0 for up to 1,000 requests/day
- **One run**: 50 LLM calls (10 models × 5 generations)
- **Experiments per day**: 20 runs within free quota
- **Perfect for**: Development, research, experiments

### Paid Tier (After free quota)
- **Input tokens**: $0.10 per 1M tokens
- **Output tokens**: $0.40 per 1M tokens
- **Typical request**: ~1,500 input tokens + ~500 output tokens
- **Cost per request**: ~$0.00035 ($0.00015 input + $0.0002 output)
- **Cost per run**: ~$0.0175 (50 × $0.00035)
- **100 experiments**: ~$1.75

### Comparison to Claude 3.5 Sonnet
- **Claude input**: $3.00 per 1M tokens (30x more expensive)
- **Claude output**: $15.00 per 1M tokens (37.5x more expensive)
- **Claude cost per request**: ~$0.012 (34x more expensive)
- **Claude cost per run**: ~$0.60 (34x more expensive)
- **100 experiments**: ~$60 (34x more expensive)

**Conclusion**: Gemini 2.5 Flash Lite is 30-40x cheaper and has free tier. Perfect match for project constraints.

---

## Next Steps

1. **Review this plan** - Confirm approach, answer open questions
2. **Create infrastructure files** - Start with Phase 1
3. **Test API connection** - Verify credentials work
4. **Implement client** - Phase 2 (gemini_client.py)
5. **Create prompts** - Phase 3 (prompts.py)
6. **Integrate** - Phase 5 (modify prototype.py)
7. **Test** - Run full evolution, debug issues
8. **Document** - Update README, create guide
9. **Iterate** - Analyze results, improve prompts
10. **Share results** - Create PR, document findings

Ready to start implementation!
